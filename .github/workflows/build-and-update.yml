name: Build and Update Beehiiv Feed with Maximum Engagement

on:
  schedule:
    # 05:40 Australia/Brisbane daily == 19:40 UTC previous day
    - cron: "40 19 * * *"
  workflow_dispatch:
    inputs:
      force_comprehensive:
        description: 'Force comprehensive fetch (ignore database state)'
        required: false
        default: 'false'
        type: boolean
      debug_mode:
        description: 'Enable debug logging for troubleshooting'
        required: false
        default: 'false'
        type: boolean

permissions:
  contents: write

concurrency:
  group: build-and-update-enhanced
  cancel-in-progress: true

jobs:
  build-and-update:
    runs-on: ubuntu-latest
    timeout-minutes: 45
    
    steps:
    - name: Checkout repo
      uses: actions/checkout@v4
      
    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"
        cache: "pip"
        cache-dependency-path: requirements.txt
        
    - name: Install dependencies
      run: pip install -r requirements.txt
      
    - name: Enhanced database state check
      id: db_check
      run: |
        echo "=== ENHANCED DATABASE STATE CHECK ==="
        
        if [[ -f "rss_items.db" ]]; then
          echo "database_exists=true" >> $GITHUB_OUTPUT
          echo "‚úÖ Database file exists"
          
          # Enhanced database content analysis
          python3 -c "
        import sqlite3
        import sys
        
        try:
            conn = sqlite3.connect('rss_items.db')
            
            # Check total items
            cur = conn.execute('SELECT COUNT(*) FROM items')
            total_items = cur.fetchone()[0]
            print(f'üìä Total items in database: {total_items}')
            
            # Check relevant items
            try:
                cur = conn.execute('SELECT COUNT(*) FROM items WHERE relevant = 1')
                relevant_count = cur.fetchone()[0]
                print(f'‚úÖ Relevant items: {relevant_count}')
                
                # Check recent relevant items
                cur = conn.execute('SELECT COUNT(*) FROM items WHERE relevant = 1 AND datetime(published_at) > datetime(\"now\", \"-24 hours\")')
                recent_relevant = cur.fetchone()[0]
                print(f'üïê Recent relevant items (24h): {recent_relevant}')
                
                # Check AI processing
                cur = conn.execute('SELECT COUNT(*) FROM items WHERE ai_title IS NOT NULL AND ai_title != \"\"')
                ai_processed = cur.fetchone()[0]
                print(f'ü§ñ AI processed items: {ai_processed}')
                
                # Check categories
                cur = conn.execute('SELECT ai_tags, COUNT(*) FROM items WHERE relevant = 1 AND ai_tags IS NOT NULL GROUP BY ai_tags')
                categories = cur.fetchall()
                print(f'üìÇ Categories found: {len(categories)}')
                for cat, count in categories[:5]:
                    print(f'   {cat}: {count} items')
                
                if relevant_count > 0:
                    print('database_populated=true')
                    echo \"database_populated=true\" >> \$GITHUB_OUTPUT
                else:
                    print('database_populated=false')
                    echo \"database_populated=false\" >> \$GITHUB_OUTPUT
                    
            except Exception as e:
                print(f'‚ùå Error checking relevant items: {e}')
                print('database_populated=false')
                echo \"database_populated=false\" >> \$GITHUB_OUTPUT
                
            conn.close()
            
        except Exception as e:
            print(f'‚ùå Database check failed: {e}')
            print('database_populated=false')
            sys.exit(0)
        " 2>&1 | tee database_check.log
          
          if grep -q "database_populated=true" database_check.log; then
            echo "database_populated=true" >> $GITHUB_OUTPUT
          else
            echo "database_populated=false" >> $GITHUB_OUTPUT
          fi
          
        else
          echo "database_exists=false" >> $GITHUB_OUTPUT
          echo "database_populated=false" >> $GITHUB_OUTPUT
          echo "‚ùå No database file found - will create fresh"
        fi
        
        echo "=== DATABASE CHECK COMPLETE ==="
        
    - name: Enhanced RSS analysis with maximum engagement
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        RSS_DB_PATH: rss_items.db
        LOG_LEVEL: ${{ github.event.inputs.debug_mode == 'true' && 'DEBUG' || 'INFO' }}
        SCAN_WINDOW_HRS: "24"
        ALWAYS_AI: "1"
        MIN_SCORE_FOR_AI: "3"  # Lower threshold for more content
        MIN_ITEMS_PER_CATEGORY: "2"
        OAI_RPM: "35"  # Slightly higher for headline generation
        OAI_MAX_RETRIES: "5"   # More retries for reliability
        FEED_TIMEOUT: "20"     # Longer timeout for problematic feeds
      run: |
        set -euxo pipefail
        
        echo "=== ENHANCED RSS ANALYSIS STARTING ==="
        echo "üîß Configuration:"
        echo "   Database exists: ${{ steps.db_check.outputs.database_exists }}"
        echo "   Database populated: ${{ steps.db_check.outputs.database_populated }}"
        echo "   Force comprehensive: ${{ github.event.inputs.force_comprehensive }}"
        echo "   Debug mode: ${{ github.event.inputs.debug_mode }}"
        echo "   Min score for AI: $MIN_SCORE_FOR_AI"
        echo "   OpenAI RPM limit: $OAI_RPM"
        
        # Set comprehensive fetch mode
        if [[ "${{ steps.db_check.outputs.database_populated }}" == "false" ]] || [[ "${{ github.event.inputs.force_comprehensive }}" == "true" ]]; then
          echo "üöÄ Will perform COMPREHENSIVE fetch for maximum content"
          export COMPREHENSIVE_FETCH=1
        else
          echo "‚ö° Will perform SMART incremental fetch"
          export COMPREHENSIVE_FETCH=0
        fi
        
        # Run enhanced analyzer
        echo "ü§ñ Starting AI-powered analysis..."
        python rss_analyzer.py
        
        # Enhanced results logging
        echo "=== PROCESSING RESULTS ==="
        python3 -c "
        import sqlite3
        from datetime import datetime, timedelta, timezone
        
        TZ = timezone(timedelta(hours=10))
        conn = sqlite3.connect('rss_items.db')
        
        try:
            # Total items
            cur = conn.execute('SELECT COUNT(*) FROM items')
            total = cur.fetchone()[0]
            
            # Relevant items  
            cur = conn.execute('SELECT COUNT(*) FROM items WHERE relevant = 1')
            relevant = cur.fetchone()[0]
            
            # Recent relevant items
            cur = conn.execute('SELECT COUNT(*) FROM items WHERE relevant = 1 AND datetime(published_at) > datetime(\"now\", \"-24 hours\")')
            recent = cur.fetchone()[0]
            
            # High engagement items
            cur = conn.execute('SELECT COUNT(*) FROM items WHERE urgency_score >= 8')
            high_engagement = cur.fetchone()[0] if cur.fetchone() else 0
            
            # Money tracked
            cur = conn.execute('SELECT total_money_tracked FROM newsletter_metadata ORDER BY created_at DESC LIMIT 1')
            money_result = cur.fetchone()
            money_tracked = money_result[0] if money_result else 0
            
            # Latest headline/subhead
            cur = conn.execute('SELECT headline, subhead FROM newsletter_metadata ORDER BY created_at DESC LIMIT 1')
            meta = cur.fetchone()
            
            print(f'üìä ANALYSIS COMPLETE:')
            print(f'   üì• Total items processed: {total}')
            print(f'   ‚úÖ Relevant items kept: {relevant}')
            print(f'   üïê Recent relevant (24h): {recent}')
            print(f'   üî• High engagement items: {high_engagement}')
            print(f'   üí∞ Money flow tracked: \${money_tracked:,.0f}M')
            print(f'')
            
            if meta:
                print(f'üéØ GENERATED CONTENT:')
                print(f'   üì∞ Headline: {meta[0]}')
                print(f'   üìù Subhead: {meta[1]}')
            else:
                print('‚ùå No headline/subhead generated - check AI processing')
                
        except Exception as e:
            print(f'‚ùå Results analysis failed: {e}')
        finally:
            conn.close()
        "
        
        echo "=== RSS ANALYSIS COMPLETE ==="

    - name: Generate enhanced Beehiiv feed
      env:
        RSS_DB_PATH: rss_items.db
        OUT_PATH: beehiiv.xml
        NEWSLETTER_NAME: "Property Intelligence Network"
        BASE_URL: "https://mwhiteoak.github.io/morningBrief"
        SCAN_WINDOW_HRS: "24"
      run: |
        set -euxo pipefail
        
        echo "=== ENHANCED FEED GENERATION ==="
        
        # Generate maximum engagement feed
        python write_beehiiv_feed.py
        
        # Validate and analyze output
        if command -v xmllint >/dev/null; then
          echo "üîç Validating XML structure..."
          xmllint --noout beehiiv.xml && echo "‚úÖ XML is well-formed" || echo "‚ùå XML validation failed"
        fi
        
        echo "üìä FEED STATISTICS:"
        echo "   Items in feed: $(grep -c '<item>' beehiiv.xml || echo '0')"
        echo "   Categories used: $(grep -o '<category>[^<]*</category>' beehiiv.xml | sort -u | wc -l || echo '0')"
        echo "   Critical items: $(grep -c 'critical-intelligence' beehiiv.xml || echo '0')"
        echo "   High priority items: $(grep -c 'high-priority' beehiiv.xml || echo '0')"
        
        # Extract and display feed metadata
        if grep -q '<title>' beehiiv.xml; then
          FEED_TITLE=$(grep -o '<title>[^<]*</title>' beehiiv.xml | head -1 | sed 's/<[^>]*>//g')
          echo "   üì∞ Feed title: $FEED_TITLE"
        fi
        
        if grep -q '<description>' beehiiv.xml; then
          FEED_DESC=$(grep -o '<description>[^<]*</description>' beehiiv.xml | head -1 | sed 's/<[^>]*>//g' | cut -c1-100)
          echo "   üìù Feed description: $FEED_DESC..."
        fi
        
        echo "=== FEED GENERATION COMPLETE ==="

    - name: Commit and push enhanced updates
      run: |
        set -euxo pipefail
        
        echo "=== COMMITTING ENHANCED UPDATES ==="
        
        # Configure git
        git config user.name "github-actions[bot]"
        git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
        
        # Add the feed file
        git add beehiiv.xml
        
        # Conditionally add database
        if [[ "${{ steps.db_check.outputs.database_exists }}" == "false" ]] || [[ "${{ github.event.inputs.force_comprehensive }}" == "true" ]]; then
          echo "üì¶ Committing database (new or comprehensive update)"
          git add rss_items.db
        fi
        
        # Check for changes
        if git diff --staged --quiet; then
          echo "‚ÑπÔ∏è No changes to commit"
        else
          # Get enhanced headline for commit message
          HEADLINE=$(python3 -c "
        import sqlite3
        try:
            conn = sqlite3.connect('rss_items.db')
            cur = conn.execute('SELECT headline FROM newsletter_metadata ORDER BY created_at DESC LIMIT 1')
            result = cur.fetchone()
            if result and result[0]:
                print(result[0][:80])  # Truncate for git
            else:
                print('üö® Property Intelligence Update')
            conn.close()
        except:
            print('üö® Property Intelligence Update')
        " 2>/dev/null || echo "üö® Property Intelligence Update")
          
          # Enhanced commit message
          git commit -m "üöÄ Enhanced Update: ${HEADLINE}" \
                     -m "Auto-generated intelligence briefing with maximum engagement" \
                     -m "‚Ä¢ AI-powered content curation and rewriting" \
                     -m "‚Ä¢ Engagement scoring and prioritization" \
                     -m "‚Ä¢ Enhanced visual presentation" \
                     -m "‚Ä¢ Generated: $(date '+%Y-%m-%d %H:%M %Z')"
          
          # Enhanced push with retry logic
          echo "üöÄ Pushing enhanced updates..."
          for i in {1..5}; do
            if git pull --rebase --autostash origin main; then
              if git push origin main; then
                echo "‚úÖ Successfully pushed enhanced updates (attempt $i)"
                break
              else
                echo "‚ùå Push failed, attempt $i/5"
                sleep $((i * 3))
              fi
            else
              echo "‚ùå Pull failed, attempt $i/5"
              sleep $((i * 3))
            fi
            
            if [[ $i -eq 5 ]]; then
              echo "üí• Failed to push after 5 attempts"
              exit 1
            fi
          done
        fi
        
        echo "=== COMMIT PROCESS COMPLETE ==="

    - name: Enhanced workflow summary
      if: always()
      run: |
        echo "üéØ === ENHANCED WORKFLOW SUMMARY ==="
        echo ""
        
        # Database status
        echo "üìä DATABASE STATUS:"
        echo "   Existed before run: ${{ steps.db_check.outputs.database_exists }}"
        echo "   Was populated: ${{ steps.db_check.outputs.database_populated }}"
        echo "   Comprehensive fetch: ${{ github.event.inputs.force_comprehensive }}"
        echo ""
        
        # Feed status
        if [[ -f "beehiiv.xml" ]]; then
          ITEM_COUNT=$(grep -c '<item>' beehiiv.xml || echo '0')
          CRITICAL_COUNT=$(grep -c 'critical-intelligence' beehiiv.xml || echo '0')
          HIGH_PRIORITY_COUNT=$(grep -c 'high-priority' beehiiv.xml || echo '0')
          
          echo "üì∞ FEED GENERATED:"
          echo "   Total items: $ITEM_COUNT"
          echo "   Critical alerts: $CRITICAL_COUNT"
          echo "   High priority: $HIGH_PRIORITY_COUNT"
          echo ""
          
          # Extract feed title and description
          if FEED_TITLE=$(grep -o '<title>[^<]*</title>' beehiiv.xml | head -1 | sed 's/<[^>]*>//g' 2>/dev/null); then
            echo "   üì∞ Title: $FEED_TITLE"
          fi
          
          if FEED_DESC=$(grep -o '<description>[^<]*</description>' beehiiv.xml | head -1 | sed 's/<[^>]*>//g' | cut -c1-150 2>/dev/null); then
            echo "   üìù Description: $FEED_DESC..."
          fi
          echo ""
        else
          echo "‚ùå NO FEED GENERATED"
          echo ""
        fi
        
        # URLs and access
        echo "üåê ACCESS INFORMATION:"
        echo "   Feed URL: https://mwhiteoak.github.io/morningBrief/beehiiv.xml"
        echo "   Repository: https://github.com/${{ github.repository }}"
        echo "   Workflow run: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}"
        echo ""
        
        # Next steps
        echo "üîÑ NEXT SCHEDULED RUN:"
        echo "   Daily at 05:40 Australia/Brisbane (19:40 UTC previous day)"
        echo "   Manual trigger available via workflow_dispatch"
        echo ""
        
        echo "‚úÖ === ENHANCED WORKFLOW COMPLETE ==="
